{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Colab notebook for Illini VEX Robotics Software Development Meeting - 1/25/2024\n",
    "kidskoding (Anirudh Konidala)"
   ],
   "id": "7a097cd158786233"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Goals for this meeting \n",
    "- [x] Understand Deep Q-Learning and how it works\n",
    "- [x] Use a sample environment from OpenAI Gym, [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/),\n",
    "   and implement Deep Q-Learning with Deep Q-Networks to train the agent to smoothly land on the moon's surface\n",
    "   \n",
    "### Next meeting: February 1st, 2024"
   ],
   "id": "47c491286202edca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Recap from last semester!\n",
    "\n",
    "**Reinforcement Learning** → A branch in Machine Learning (ML) where an agent learns to make decisions by interacting with an environment in order to maximize some cumulative reward.\n",
    "- **Agent** → The decision maker that interacts with the environment\n",
    "- **Environment** → The space or system the agent operates in and responds to said agent’s actions\n",
    "- **State** → A representation of the environment’s current situation\n",
    "- **Action** → Choices the agent can make that affect the environment\n",
    "- **Reward** → A numerical value received after each action, indicating the desirability of the outcome\n",
    "\n",
    "Reinforcement Learning balances **exploration** → trying new actions to discover their effects) and **exploitation** → choosing actions that are known to yield high rewards.\n",
    "\n",
    "The classic Atari game, Pong, is one example of Reinforcement Learning.\n",
    "\n",
    "- The **agent** would be the paddle that is controlled by the AI\n",
    "- The **environment** would be the Pong game screen\n",
    "- The **state** would be the position of the ball and both paddles\n",
    "- The **action** would involve moving the paddle up, down, or keeping it stationary\n",
    "- The **reward** would be Positive for scoring a point and negative for losing a point"
   ],
   "id": "8f64f87f1577f78c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Action Space** → The set of all possible actions an agent can take in a given environment\n",
    "   - **Discrete Action Space** → A finite set of actions (e.g., moving left or right)\n",
    "\n",
    "**Observation Space** → The set of all possible states an agent can observe in a given environment"
   ],
   "id": "9853a1c7df3d3850"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Deep Q-Learning\n",
    "\n",
    "**Deep Q-Learning (DQN)** is an extension of Q-Learning that focuses on using deep neural networks to approximate the Q-value\n",
    "function (action-value function)\n",
    "- The **Q-value function** is a function that takes in a state and action as input and outputs the expected cumulative reward, \n",
    "measuring how good an action is for an agent in a given state\n",
    "    - Implemented using **Neural Networks (NNs)** or **Deep Q-Networks (DQNs)**\n",
    "\n",
    "### Key Components\n",
    "- **Q-network** → a neural network that takes the state as input and outputs Q-values for each possible action\n",
    "- **Target Network** → a copy of the Q-network that acts as a reference for training the Q-network itself\n",
    "    - stabilizes the training process for the DQN by preventing large fluctuations or changes in performance during training\n",
    "- **Experience Replay** → An RL technique where agents can memorize and reuse past experiences to improve learning\n",
    "    - Implemented via a **Replay Buffer** → a data structure (typically a deque) that stores the agent's past experiences \n",
    "    (typically state, action, reward, next state), allowing it to randomly sample and reuse these experience during its training"
   ],
   "id": "c8f974cb04cf7356"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T21:12:37.622040Z",
     "start_time": "2025-02-01T21:12:37.160596Z"
    }
   },
   "cell_type": "code",
   "source": "import gymnasium as gym",
   "id": "73384f89ff31d237",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Important Parameters** → These hyperparameters control the behavior and performance of the DQN\n",
    "- **BUFFER_SIZE** → The maximum size of the replay buffer, which stores past experiences for training the Q-network.\n",
    "- **BATCH_SIZE** → The number of experiences sampled from the replay buffer to train the Q-network in each training step.\n",
    "- **GAMMA** → The discount factor used in the Q-learning update rule, which determines the importance of future rewards.\n",
    "- **LR** → The learning rate for the optimizer, which controls how much to adjust the Q-network's weights with respect to the loss gradient.\n",
    "- **EPSILON** → The initial value of epsilon for the epsilon-greedy policy, which determines the probability of choosing a random action versus the action suggested by the Q-network.\n",
    "- **EPSILON_MIN** → The minimum value of epsilon, ensuring that there is always some probability of choosing a random action.\n",
    "- **EPSILON_DECAY** → The decay rate for epsilon, which reduces epsilon after each episode to decrease the probability of choosing random actions over time.\n",
    "- **TARGET_UPDATE_FREQ** → The frequency (in episodes) at which the target Q-network is updated with the weights of the current Q-network."
   ],
   "id": "f5f3ca9531578dc0"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-01T21:19:02.049905Z",
     "start_time": "2025-02-01T21:19:02.045400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "TARGET_UPDATE_FREQ = 10"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create the environment for training the agent by using the Lunar Lander environment from OpenAI Gym!",
   "id": "f1b4335f30034c0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T21:21:33.714163Z",
     "start_time": "2025-02-01T21:21:33.709704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "env.action_space.seed(42)\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = 4"
   ],
   "id": "6619b50063995017",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Create the Q-Network implementation, which is derived from the Neural Network module in the PyTorch library\n",
    "\n",
    "- The Q-Network consists of three fully connected layers (fc1, fc2, fc3) that map the input state to the output action\n",
    "    - The **input layer** takes the input state and maps it to a higher dimensional space -> Helps the network learn initial features from the input data\n",
    "    - The **hidden layer** processes features learned by the first layer, allowing the DQN to extract more complex features and optimize the network's weights better\n",
    "    - The **output layer** produces the Q-values for each possible action in the action space, which is equal to the number of possible actions\n",
    "- The forward function defines the forward pass of the network, or **forward propogation**, \n",
    "where the input state is being passed through the DQN layers to produce the Q-values for each action\n",
    "    - The DQN makes predictions based on the input data"
   ],
   "id": "e66058e2049e4bef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ],
   "id": "2c5bd040b8c83cef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initialize the two neural networks for a DQN algorithm: the **Q-network** and the **target network**",
   "id": "913a0762e138fd53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T21:29:12.953369Z",
     "start_time": "2025-02-01T21:29:12.941709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q_network = QNetwork(input_dim, output_dim)\n",
    "target_network = QNetwork(input_dim, output_dim)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "target_network.eval()"
   ],
   "id": "1892b42a12a9d51a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (fc1): Linear(in_features=8, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Initialize the optimizer and loss function for training the Q-network in a DQN algorithm\n",
    "\n",
    "- **optimizer** - Initializes the **Adam (Adaptive Moment Estimation) optimizer** with the DQN's parameters to update the weights of the Q-network based \n",
    "on the loss calculated during training\n",
    "    - Adam adjusts the learning rate for each parameter based on the first and second moments of the gradients\n",
    "- **loss function** - Initializes the **Mean Squared Error (MSE) loss function** to calculate the difference between the predicted Q-values \n",
    "and the target Q-values, which computes the gradients for backpropogation"
   ],
   "id": "ced83a44a781bdb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T21:35:46.681202Z",
     "start_time": "2025-02-01T21:35:45.737977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=LR)\n",
    "loss_fn = nn.MSELoss()"
   ],
   "id": "15e2e0d3917111cc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Initialize the replay buffer, which stores past experiences for training the Q-network in a DQN algorithm\n",
    "\n",
    "- Done via an implementation of the **deque (double ended queue)** data structure\n",
    "    - The store function appends new experiences to the buffer deque data structure\n",
    "    - The sample function takes a random sample of experiences from the buffer for training the Q-network\n",
    "    - The size function returns the current size of the buffer"
   ],
   "id": "eaee8d554bd1ab4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T21:51:00.419531Z",
     "start_time": "2025-02-01T21:51:00.415475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "    def store(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    def sample(self):\n",
    "        return random.sample(self.buffer, self.batch_size)\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "replay_buffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)"
   ],
   "id": "fc7b562c0901dc59",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Initialize the environment and print information about the observation and action spaces\n",
    "\n",
    "1. **Reset** the environment to obtain the initial state and information\n",
    "2. Print the **observation space** and **action space** of the environment\n",
    "3. Print a sample observation from the observation space\n",
    "4. **Observation** - The current state of the environment, which includes the agent's position, velocity, angle, and leg contact with the ground"
   ],
   "id": "9fe661cfa3b979c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### In our Lunar Lander environment, there are a total of 8 observations\n",
    "\n",
    "- [0] x-coordinate\n",
    "- [1] y-coordinate\n",
    "- [2] x-velocity\n",
    "- [3] y-velocity\n",
    "- [4] angle\n",
    "- [5] angular velocity\n",
    "- [6] left leg touching ground\n",
    "- [7] right leg touching ground\n",
    "\n",
    "### and a total of 4 actions\n",
    "- 0 - do nothing\n",
    "- 1 - left engine\n",
    "- 2 - main engine\n",
    "- 3 - right engine"
   ],
   "id": "59bae87d284e32af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T21:51:03.600464Z",
     "start_time": "2025-02-01T21:51:03.595358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "observation, info = env.reset(seed=42)\n",
    "print(\"env.observation_space\", env.observation_space)\n",
    "print(\"env.action_space\", env.action_space)\n",
    "print(\"env.observation_space.sample()\", env.observation_space.sample())"
   ],
   "id": "47e763d62a1f0c5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.observation_space Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ], (8,), float32)\n",
      "env.action_space Discrete(4)\n",
      "env.observation_space.sample() [ 0.8571803  -2.0454006  -7.4847665  -8.4603815  -0.5257534  -2.0264833\n",
      "  0.4582417   0.45993498]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the Lunar Lander environment for a specified number of episodes using the DQN algorithm!",
   "id": "54a68e13de8bb6c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_episodes = 500\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment and get the initial state\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    total_reward = 0\n",
    "\n",
    "    # Select an action using the epsilon-greedy approach\n",
    "    # - Randomly select an action with the probability of epsilon\n",
    "    # - Otherwise, select the action with the highest Q-value from the Q-network\n",
    "    while True:\n",
    "        if random.random() < EPSILON:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(q_network(state)).item()\n",
    "\n",
    "        # Take the action and observe the next state and reward\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Store the experience in the replay buffer\n",
    "        replay_buffer.store((state, action, reward, next_state, done))\n",
    "\n",
    "        # Update the state to the next state and total reward to accumulate the reward from the current experience\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Check if the replay buffer hasn't exceed capacity and enough experience to sample another batch\n",
    "        if replay_buffer.size() >= BATCH_SIZE:\n",
    "            # Decompose the tuple containing the random sample of the experience from the replay buffer\n",
    "            batch = replay_buffer.sample()\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            # Convert the experience into PyTorch tensors for training the Deep Q Network\n",
    "            states = torch.stack(states)\n",
    "            actions = torch.tensor(actions).unsqueeze(1)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "            next_states = torch.stack(next_states)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "            \n",
    "            # Calculate the Q-values for the current state and action from the Q-network, and then\n",
    "            # compute the target Q-values using the target network\n",
    "            q_values = q_network(states).gather(1, actions)\n",
    "            with torch.no_grad():\n",
    "                max_next_q_values = target_network(next_states).max(1, keepdim=True)[0]\n",
    "                target_q_values = rewards + GAMMA * max_next_q_values * (1 - dones)\n",
    "            \n",
    "            # Compute the loss between the predicted Q-values and target Q-values\n",
    "            loss = loss_fn(q_values, target_q_values)\n",
    "            \n",
    "            # 1. Reset the gradients of all the parameters to zero\n",
    "            # 2. Backpropogate the loss to compute the gradients\n",
    "            # 3. Update the weights of the Q-network using the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Exit the simulation when finished!\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Update epsilon using the decay rate -> reduces the probability of choosing a random action over time, \n",
    "    # encouraging exploitation of learned actions as the training progresses\n",
    "    EPSILON = max(EPSILON_MIN, EPSILON * EPSILON_DECAY)\n",
    "    \n",
    "    # Update the target network with the weights of the Q-network at a specified frequency\n",
    "    if episode % TARGET_UPDATE_FREQ == 0:\n",
    "        target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    # Print the episode number, total reward, and epsilon value\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {EPSILON:.3f}\")"
   ],
   "id": "80f92e04b3ac2363"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### TODO for next meeting\n",
    "- [ ] Continue to gain a greater understanding of Deep Q-Learning, along with how and why \n",
    "it makes the landing of the agent on the moon's surface much more smoother\n",
    "    - Specifically focus on \n",
    "        - [ ] Backpropogation\n",
    "        - [ ] Loss function\n",
    "- [ ] Understand the code and why it works\n",
    "- [ ] Perhaps have a brief look at Rainbow DQN??\n",
    "- [ ] Begin implementing our own environment, possibly in Unity with a simple environment of soccer?"
   ],
   "id": "1bf81fc867eb023b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
