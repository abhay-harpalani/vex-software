{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Colab notebook for Illini VEX Robotics Software Development Meeting - 1/25/2024\n",
    "kidskoding (Anirudh Konidala)"
   ],
   "id": "7a097cd158786233"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Goals for this meeting \n",
    "- [x] Understand Deep Q-Learning and how it works\n",
    "- [x] Use a sample environment from OpenAI Gym, [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/),\n",
    "   and implement Deep Q-Learning with Deep Q-Networks to train the agent to smoothly land on the moon's surface\n",
    "   \n",
    "### Next meeting: February 1st, 2024"
   ],
   "id": "47c491286202edca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Recap from last semester!\n",
    "\n",
    "**Reinforcement Learning** → A branch in Machine Learning (ML) where an agent learns to make decisions by interacting with an environment in order to maximize some cumulative reward.\n",
    "- **Agent** → The decision maker that interacts with the environment\n",
    "- **Environment** → The space or system the agent operates in and responds to said agent’s actions\n",
    "- **State** → A representation of the environment’s current situation\n",
    "- **Action** → Choices the agent can make that affect the environment\n",
    "- **Reward** → A numerical value received after each action, indicating the desirability of the outcome\n",
    "\n",
    "Reinforcement Learning balances **exploration** → trying new actions to discover their effects) and **exploitation** → choosing actions that are known to yield high rewards.\n",
    "\n",
    "The classic Atari game, Pong, is one example of Reinforcement Learning.\n",
    "\n",
    "- The **agent** would be the paddle that is controlled by the AI\n",
    "- The **environment** would be the Pong game screen\n",
    "- The **state** would be the position of the ball and both paddles\n",
    "- The **action** would involve moving the paddle up, down, or keeping it stationary\n",
    "- The **reward** would be Positive for scoring a point and negative for losing a point"
   ],
   "id": "8f64f87f1577f78c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Action Space** → The set of all possible actions an agent can take in a given environment\n",
    "   - **Discrete Action Space** → A finite set of actions (e.g., moving left or right)\n",
    "\n",
    "**Observation Space** → The set of all possible states an agent can observe in a given environment"
   ],
   "id": "9853a1c7df3d3850"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Deep Q-Learning\n",
    "\n",
    "**Deep Q-Learning (DQN)** is an extension of Q-Learning that focuses on using deep neural networks to approximate the Q-value\n",
    "function (action-value function)\n",
    "- The **Q-value function** is a function that takes in a state and action as input and outputs the expected cumulative reward, \n",
    "measuring how good an action is for an agent in a given state\n",
    "    - Implemented using **Neural Networks (NNs)** or **Deep Q-Networks (DQNs)**\n",
    "\n",
    "### Key Components\n",
    "- **Q-network** → a neural network that takes the state as input and outputs Q-values for each possible action\n",
    "- **Target Network** → a copy of the Q-network that acts as a reference for training the Q-network itself\n",
    "    - stabilizes the training process for the DQN by preventing large fluctuations or changes in performance during training\n",
    "- **Experience Replay** → An RL technique where agents can memorize and reuse past experiences to improve learning\n",
    "    - Implemented via a **Replay Buffer** → a data structure (typically a deque) that stores the agent's past experiences \n",
    "    (typically state, action, reward, next state), allowing it to randomly sample and reuse these experience during its training"
   ],
   "id": "c8f974cb04cf7356"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T22:42:26.059856Z",
     "start_time": "2025-02-01T22:42:25.757925Z"
    }
   },
   "cell_type": "code",
   "source": "import gymnasium as gym",
   "id": "73384f89ff31d237",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Important Parameters** → These hyperparameters control the behavior and performance of the DQN\n",
    "- **BUFFER_SIZE** → The maximum size of the replay buffer, which stores past experiences for training the Q-network.\n",
    "- **BATCH_SIZE** → The number of experiences sampled from the replay buffer to train the Q-network in each training step.\n",
    "- **GAMMA** → The discount factor used in the Q-learning update rule, which determines the importance of future rewards.\n",
    "- **LR** → The learning rate for the optimizer, which controls how much to adjust the Q-network's weights with respect to the loss gradient.\n",
    "- **EPSILON** → The initial value of epsilon for the epsilon-greedy policy, which determines the probability of choosing a random action versus the action suggested by the Q-network.\n",
    "- **EPSILON_MIN** → The minimum value of epsilon, ensuring that there is always some probability of choosing a random action.\n",
    "- **EPSILON_DECAY** → The decay rate for epsilon, which reduces epsilon after each episode to decrease the probability of choosing random actions over time.\n",
    "- **TARGET_UPDATE_FREQ** → The frequency (in episodes) at which the target Q-network is updated with the weights of the current Q-network."
   ],
   "id": "f5f3ca9531578dc0"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-01T22:42:26.071076Z",
     "start_time": "2025-02-01T22:42:26.068189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "TARGET_UPDATE_FREQ = 10"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create the environment for training the agent by using the Lunar Lander environment from OpenAI Gym!",
   "id": "f1b4335f30034c0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T22:42:26.185173Z",
     "start_time": "2025-02-01T22:42:26.080308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "env.action_space.seed(42)\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = 4"
   ],
   "id": "6619b50063995017",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Create the Q-Network implementation, which is derived from the Neural Network module in the PyTorch library\n",
    "\n",
    "- The Q-Network consists of three fully connected layers (fc1, fc2, fc3) that map the input state to the output action\n",
    "    - The **input layer** takes the input state and maps it to a higher dimensional space -> Helps the network learn initial features from the input data\n",
    "    - The **hidden layer** processes features learned by the first layer, allowing the DQN to extract more complex features and optimize the network's weights better\n",
    "    - The **output layer** produces the Q-values for each possible action in the action space, which is equal to the number of possible actions\n",
    "- The forward function defines the forward pass of the network, or **forward propogation**, \n",
    "where the input state is being passed through the DQN layers to produce the Q-values for each action\n",
    "    - The DQN makes predictions based on the input data"
   ],
   "id": "e66058e2049e4bef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T22:42:27.627255Z",
     "start_time": "2025-02-01T22:42:26.193520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ],
   "id": "2c5bd040b8c83cef",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initialize the two neural networks for a DQN algorithm: the **Q-network** and the **target network**",
   "id": "913a0762e138fd53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T22:42:27.654346Z",
     "start_time": "2025-02-01T22:42:27.637430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q_network = QNetwork(input_dim, output_dim)\n",
    "target_network = QNetwork(input_dim, output_dim)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "target_network.eval()"
   ],
   "id": "1892b42a12a9d51a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (fc1): Linear(in_features=8, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Initialize the optimizer and loss function for training the Q-network in a DQN algorithm\n",
    "\n",
    "- **optimizer** - Initializes the **Adam (Adaptive Moment Estimation) optimizer** with the DQN's parameters to update the weights of the Q-network based \n",
    "on the loss calculated during training\n",
    "    - Adam adjusts the learning rate for each parameter based on the first and second moments of the gradients\n",
    "- **loss function** - Initializes the **Mean Squared Error (MSE) loss function** to calculate the difference between the predicted Q-values \n",
    "and the target Q-values, which computes the gradients for backpropogation"
   ],
   "id": "ced83a44a781bdb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T22:42:28.485080Z",
     "start_time": "2025-02-01T22:42:27.664600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=LR)\n",
    "loss_fn = nn.MSELoss()"
   ],
   "id": "15e2e0d3917111cc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Initialize the replay buffer, which stores past experiences for training the Q-network in a DQN algorithm\n",
    "\n",
    "- Done via an implementation of the **deque (double ended queue)** data structure\n",
    "    - The store function appends new experiences to the buffer deque data structure\n",
    "    - The sample function takes a random sample of experiences from the buffer for training the Q-network\n",
    "    - The size function returns the current size of the buffer"
   ],
   "id": "eaee8d554bd1ab4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T22:42:28.495573Z",
     "start_time": "2025-02-01T22:42:28.491878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "    def store(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    def sample(self):\n",
    "        return random.sample(self.buffer, self.batch_size)\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "replay_buffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)"
   ],
   "id": "fc7b562c0901dc59",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Initialize the environment and print information about the observation and action spaces\n",
    "\n",
    "1. **Reset** the environment to obtain the initial state and information\n",
    "2. Print the **observation space** and **action space** of the environment\n",
    "3. Print a sample observation from the observation space\n",
    "4. **Observation** - The current state of the environment, which includes the agent's position, velocity, angle, and leg contact with the ground"
   ],
   "id": "9fe661cfa3b979c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### In our Lunar Lander environment, there are a total of 8 observations\n",
    "\n",
    "- [0] x-coordinate\n",
    "- [1] y-coordinate\n",
    "- [2] x-velocity\n",
    "- [3] y-velocity\n",
    "- [4] angle\n",
    "- [5] angular velocity\n",
    "- [6] left leg touching ground\n",
    "- [7] right leg touching ground\n",
    "\n",
    "### and a total of 4 actions\n",
    "- 0 - do nothing\n",
    "- 1 - left engine\n",
    "- 2 - main engine\n",
    "- 3 - right engine"
   ],
   "id": "59bae87d284e32af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T22:42:28.510753Z",
     "start_time": "2025-02-01T22:42:28.503964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "observation, info = env.reset(seed=42)\n",
    "print(\"env.observation_space\", env.observation_space)\n",
    "print(\"env.action_space\", env.action_space)\n",
    "print(\"env.observation_space.sample()\", env.observation_space.sample())"
   ],
   "id": "47e763d62a1f0c5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.observation_space Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ], (8,), float32)\n",
      "env.action_space Discrete(4)\n",
      "env.observation_space.sample() [ 1.1708826  -0.20747724 -9.526636    6.988718    4.605588    8.106973\n",
      "  0.8010677   0.42955056]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the Lunar Lander environment for a specified number of episodes using the DQN algorithm!",
   "id": "54a68e13de8bb6c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T22:43:56.997081Z",
     "start_time": "2025-02-01T22:42:28.598729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_episodes = 500\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment and get the initial state\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    total_reward = 0\n",
    "\n",
    "    # Select an action using the epsilon-greedy approach\n",
    "    # - Randomly select an action with the probability of epsilon\n",
    "    # - Otherwise, select the action with the highest Q-value from the Q-network\n",
    "    while True:\n",
    "        if random.random() < EPSILON:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(q_network(state)).item()\n",
    "\n",
    "        # Take the action and observe the next state and reward\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Store the experience in the replay buffer\n",
    "        replay_buffer.store((state, action, reward, next_state, done))\n",
    "\n",
    "        # Update the state to the next state and total reward to accumulate the reward from the current experience\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # Check if the replay buffer hasn't exceed capacity and enough experience to sample another batch\n",
    "        if replay_buffer.size() >= BATCH_SIZE:\n",
    "            # Decompose the tuple containing the random sample of the experience from the replay buffer\n",
    "            batch = replay_buffer.sample()\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            # Convert the experience into PyTorch tensors for training the Deep Q Network\n",
    "            states = torch.stack(states)\n",
    "            actions = torch.tensor(actions).unsqueeze(1)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "            next_states = torch.stack(next_states)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "            \n",
    "            # Calculate the Q-values for the current state and action from the Q-network, and then\n",
    "            # compute the target Q-values using the target network\n",
    "            q_values = q_network(states).gather(1, actions)\n",
    "            with torch.no_grad():\n",
    "                max_next_q_values = target_network(next_states).max(1, keepdim=True)[0]\n",
    "                target_q_values = rewards + GAMMA * max_next_q_values * (1 - dones)\n",
    "            \n",
    "            # Compute the loss between the predicted Q-values and target Q-values\n",
    "            loss = loss_fn(q_values, target_q_values)\n",
    "            \n",
    "            # 1. Reset the gradients of all the parameters to zero\n",
    "            # 2. Backpropogate the loss to compute the gradients\n",
    "            # 3. Update the weights of the Q-network using the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Exit the simulation when finished!\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Update epsilon using the decay rate -> reduces the probability of choosing a random action over time, \n",
    "    # encouraging exploitation of learned actions as the training progresses\n",
    "    EPSILON = max(EPSILON_MIN, EPSILON * EPSILON_DECAY)\n",
    "    \n",
    "    # Update the target network with the weights of the Q-network at a specified frequency\n",
    "    if episode % TARGET_UPDATE_FREQ == 0:\n",
    "        target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    # Print the episode number, total reward, and epsilon value\n",
    "    print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {EPSILON:.3f}\")"
   ],
   "id": "80f92e04b3ac2363",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -108.55402945184308, Epsilon: 0.995\n",
      "Episode 1, Total Reward: -117.45104501370861, Epsilon: 0.990\n",
      "Episode 2, Total Reward: -300.19288665142597, Epsilon: 0.985\n",
      "Episode 3, Total Reward: -142.596640661565, Epsilon: 0.980\n",
      "Episode 4, Total Reward: -121.25163631488947, Epsilon: 0.975\n",
      "Episode 5, Total Reward: -71.28964439970208, Epsilon: 0.970\n",
      "Episode 6, Total Reward: -130.8511766057116, Epsilon: 0.966\n",
      "Episode 7, Total Reward: -193.55462274086332, Epsilon: 0.961\n",
      "Episode 8, Total Reward: -309.7665704601866, Epsilon: 0.956\n",
      "Episode 9, Total Reward: -308.31168333791265, Epsilon: 0.951\n",
      "Episode 10, Total Reward: -120.60226100485605, Epsilon: 0.946\n",
      "Episode 11, Total Reward: -310.3328119463391, Epsilon: 0.942\n",
      "Episode 12, Total Reward: -74.45017710855774, Epsilon: 0.937\n",
      "Episode 13, Total Reward: -21.242458145776183, Epsilon: 0.932\n",
      "Episode 14, Total Reward: -45.231820858939024, Epsilon: 0.928\n",
      "Episode 15, Total Reward: -107.60697033107132, Epsilon: 0.923\n",
      "Episode 16, Total Reward: -122.03314649696262, Epsilon: 0.918\n",
      "Episode 17, Total Reward: -88.02758716189444, Epsilon: 0.914\n",
      "Episode 18, Total Reward: -133.17196094359596, Epsilon: 0.909\n",
      "Episode 19, Total Reward: -248.5816584618837, Epsilon: 0.905\n",
      "Episode 20, Total Reward: -259.86745317534917, Epsilon: 0.900\n",
      "Episode 21, Total Reward: -80.81297617846462, Epsilon: 0.896\n",
      "Episode 22, Total Reward: -224.19768055344542, Epsilon: 0.891\n",
      "Episode 23, Total Reward: -320.01160965403807, Epsilon: 0.887\n",
      "Episode 24, Total Reward: -110.15893324099427, Epsilon: 0.882\n",
      "Episode 25, Total Reward: -96.82843267783713, Epsilon: 0.878\n",
      "Episode 26, Total Reward: -117.50504997329602, Epsilon: 0.873\n",
      "Episode 27, Total Reward: -148.64552676599368, Epsilon: 0.869\n",
      "Episode 28, Total Reward: -102.90532710557409, Epsilon: 0.865\n",
      "Episode 29, Total Reward: -287.17628359543016, Epsilon: 0.860\n",
      "Episode 30, Total Reward: -33.42644825338368, Epsilon: 0.856\n",
      "Episode 31, Total Reward: -117.8936250031972, Epsilon: 0.852\n",
      "Episode 32, Total Reward: -214.17713221464885, Epsilon: 0.848\n",
      "Episode 33, Total Reward: -185.3945060992395, Epsilon: 0.843\n",
      "Episode 34, Total Reward: -103.2471905206981, Epsilon: 0.839\n",
      "Episode 35, Total Reward: -93.92266580412836, Epsilon: 0.835\n",
      "Episode 36, Total Reward: -120.26774486511545, Epsilon: 0.831\n",
      "Episode 37, Total Reward: -159.29833284935916, Epsilon: 0.827\n",
      "Episode 38, Total Reward: -114.0669092590312, Epsilon: 0.822\n",
      "Episode 39, Total Reward: -125.32881051750945, Epsilon: 0.818\n",
      "Episode 40, Total Reward: -169.2318703124822, Epsilon: 0.814\n",
      "Episode 41, Total Reward: -59.880990257432664, Epsilon: 0.810\n",
      "Episode 42, Total Reward: -305.9426319972728, Epsilon: 0.806\n",
      "Episode 43, Total Reward: -150.97450260898577, Epsilon: 0.802\n",
      "Episode 44, Total Reward: -53.040651146416906, Epsilon: 0.798\n",
      "Episode 45, Total Reward: -81.02393687914885, Epsilon: 0.794\n",
      "Episode 46, Total Reward: -104.58177352410051, Epsilon: 0.790\n",
      "Episode 47, Total Reward: -207.53472303808107, Epsilon: 0.786\n",
      "Episode 48, Total Reward: -67.90336401480691, Epsilon: 0.782\n",
      "Episode 49, Total Reward: -73.94225619149972, Epsilon: 0.778\n",
      "Episode 50, Total Reward: -206.2706952130961, Epsilon: 0.774\n",
      "Episode 51, Total Reward: -157.4002812179556, Epsilon: 0.771\n",
      "Episode 52, Total Reward: -116.2791614128578, Epsilon: 0.767\n",
      "Episode 53, Total Reward: -62.95924522444096, Epsilon: 0.763\n",
      "Episode 54, Total Reward: -168.26073496543256, Epsilon: 0.759\n",
      "Episode 55, Total Reward: -121.82838250220014, Epsilon: 0.755\n",
      "Episode 56, Total Reward: -163.80915050450514, Epsilon: 0.751\n",
      "Episode 57, Total Reward: -87.63285517012265, Epsilon: 0.748\n",
      "Episode 58, Total Reward: -134.33797859262722, Epsilon: 0.744\n",
      "Episode 59, Total Reward: -86.98954048665203, Epsilon: 0.740\n",
      "Episode 60, Total Reward: -198.8825875249792, Epsilon: 0.737\n",
      "Episode 61, Total Reward: -305.92823990910875, Epsilon: 0.733\n",
      "Episode 62, Total Reward: -95.1069758883055, Epsilon: 0.729\n",
      "Episode 63, Total Reward: -174.3463867881555, Epsilon: 0.726\n",
      "Episode 64, Total Reward: -94.3080047548572, Epsilon: 0.722\n",
      "Episode 65, Total Reward: -51.79832167940377, Epsilon: 0.718\n",
      "Episode 66, Total Reward: -215.22227487421577, Epsilon: 0.715\n",
      "Episode 67, Total Reward: -97.67715414073811, Epsilon: 0.711\n",
      "Episode 68, Total Reward: -86.88833083850591, Epsilon: 0.708\n",
      "Episode 69, Total Reward: -72.7493541953354, Epsilon: 0.704\n",
      "Episode 70, Total Reward: -164.49612350189452, Epsilon: 0.701\n",
      "Episode 71, Total Reward: -89.02189386284864, Epsilon: 0.697\n",
      "Episode 72, Total Reward: -48.906195463045925, Epsilon: 0.694\n",
      "Episode 73, Total Reward: -74.35610818004835, Epsilon: 0.690\n",
      "Episode 74, Total Reward: -119.07028578907796, Epsilon: 0.687\n",
      "Episode 75, Total Reward: -65.64556009827716, Epsilon: 0.683\n",
      "Episode 76, Total Reward: -232.2298349379947, Epsilon: 0.680\n",
      "Episode 77, Total Reward: -120.56171587720057, Epsilon: 0.676\n",
      "Episode 78, Total Reward: 16.207552975605097, Epsilon: 0.673\n",
      "Episode 79, Total Reward: -22.127666215687213, Epsilon: 0.670\n",
      "Episode 80, Total Reward: 9.522051500213493, Epsilon: 0.666\n",
      "Episode 81, Total Reward: -112.27577860390242, Epsilon: 0.663\n",
      "Episode 82, Total Reward: -74.52730100265352, Epsilon: 0.660\n",
      "Episode 83, Total Reward: -48.792380184248984, Epsilon: 0.656\n",
      "Episode 84, Total Reward: -24.40686303600384, Epsilon: 0.653\n",
      "Episode 85, Total Reward: -145.81253805161333, Epsilon: 0.650\n",
      "Episode 86, Total Reward: -79.51668737390108, Epsilon: 0.647\n",
      "Episode 87, Total Reward: -191.7478992464712, Epsilon: 0.643\n",
      "Episode 88, Total Reward: -94.65641658569153, Epsilon: 0.640\n",
      "Episode 89, Total Reward: -96.53174478457582, Epsilon: 0.637\n",
      "Episode 90, Total Reward: -64.29479548643732, Epsilon: 0.634\n",
      "Episode 91, Total Reward: -2.83560214229702, Epsilon: 0.631\n",
      "Episode 92, Total Reward: -96.91610367899558, Epsilon: 0.627\n",
      "Episode 93, Total Reward: 8.764400614174491, Epsilon: 0.624\n",
      "Episode 94, Total Reward: -79.0591704358065, Epsilon: 0.621\n",
      "Episode 95, Total Reward: -20.90012991470617, Epsilon: 0.618\n",
      "Episode 96, Total Reward: -28.304706732724554, Epsilon: 0.615\n",
      "Episode 97, Total Reward: -77.66295106615596, Epsilon: 0.612\n",
      "Episode 98, Total Reward: -219.17491452747998, Epsilon: 0.609\n",
      "Episode 99, Total Reward: -76.49996356405865, Epsilon: 0.606\n",
      "Episode 100, Total Reward: -82.93263770501046, Epsilon: 0.603\n",
      "Episode 101, Total Reward: 34.479171951547215, Epsilon: 0.600\n",
      "Episode 102, Total Reward: -125.67241257762281, Epsilon: 0.597\n",
      "Episode 103, Total Reward: -106.56589401954386, Epsilon: 0.594\n",
      "Episode 104, Total Reward: -83.89709790830584, Epsilon: 0.591\n",
      "Episode 105, Total Reward: -6.910691992414826, Epsilon: 0.588\n",
      "Episode 106, Total Reward: -70.0751441106153, Epsilon: 0.585\n",
      "Episode 107, Total Reward: -57.865984228847054, Epsilon: 0.582\n",
      "Episode 108, Total Reward: 10.761326678489553, Epsilon: 0.579\n",
      "Episode 109, Total Reward: -84.80045909778977, Epsilon: 0.576\n",
      "Episode 110, Total Reward: 1.0036848863214658, Epsilon: 0.573\n",
      "Episode 111, Total Reward: -87.29814854290262, Epsilon: 0.570\n",
      "Episode 112, Total Reward: -55.811104382185434, Epsilon: 0.568\n",
      "Episode 113, Total Reward: -66.70734310612043, Epsilon: 0.565\n",
      "Episode 114, Total Reward: -284.6914066562199, Epsilon: 0.562\n",
      "Episode 115, Total Reward: 7.904199607066175, Epsilon: 0.559\n",
      "Episode 116, Total Reward: -50.00363610215156, Epsilon: 0.556\n",
      "Episode 117, Total Reward: -48.613319530800396, Epsilon: 0.554\n",
      "Episode 118, Total Reward: -4.605819519883511, Epsilon: 0.551\n",
      "Episode 119, Total Reward: -22.881522122335042, Epsilon: 0.548\n",
      "Episode 120, Total Reward: -77.71284065578803, Epsilon: 0.545\n",
      "Episode 121, Total Reward: -91.84345448442045, Epsilon: 0.543\n",
      "Episode 122, Total Reward: -107.38487949570522, Epsilon: 0.540\n",
      "Episode 123, Total Reward: -28.155057175104147, Epsilon: 0.537\n",
      "Episode 124, Total Reward: 28.447916715033784, Epsilon: 0.534\n",
      "Episode 125, Total Reward: -99.77422251857732, Epsilon: 0.532\n",
      "Episode 126, Total Reward: -17.87765794977973, Epsilon: 0.529\n",
      "Episode 127, Total Reward: -91.22865027165119, Epsilon: 0.526\n",
      "Episode 128, Total Reward: -121.16162406928005, Epsilon: 0.524\n",
      "Episode 129, Total Reward: 4.6236446225967, Epsilon: 0.521\n",
      "Episode 130, Total Reward: 22.400207090430584, Epsilon: 0.519\n",
      "Episode 131, Total Reward: -48.031884565114765, Epsilon: 0.516\n",
      "Episode 132, Total Reward: -72.5605886458209, Epsilon: 0.513\n",
      "Episode 133, Total Reward: -69.11055162758798, Epsilon: 0.511\n",
      "Episode 134, Total Reward: -48.140012685719775, Epsilon: 0.508\n",
      "Episode 135, Total Reward: -83.92068683705659, Epsilon: 0.506\n",
      "Episode 136, Total Reward: -32.77348720757898, Epsilon: 0.503\n",
      "Episode 137, Total Reward: -69.57839352854718, Epsilon: 0.501\n",
      "Episode 138, Total Reward: -80.99884122151877, Epsilon: 0.498\n",
      "Episode 139, Total Reward: -63.50043320922461, Epsilon: 0.496\n",
      "Episode 140, Total Reward: -149.39674686360723, Epsilon: 0.493\n",
      "Episode 141, Total Reward: -81.4542924243774, Epsilon: 0.491\n",
      "Episode 142, Total Reward: -44.236334884185226, Epsilon: 0.488\n",
      "Episode 143, Total Reward: 50.600337719863376, Epsilon: 0.486\n",
      "Episode 144, Total Reward: -283.37902513702426, Epsilon: 0.483\n",
      "Episode 145, Total Reward: -348.4019833724324, Epsilon: 0.481\n",
      "Episode 146, Total Reward: -24.241450175561823, Epsilon: 0.479\n",
      "Episode 147, Total Reward: -73.38870430927628, Epsilon: 0.476\n",
      "Episode 148, Total Reward: -63.735368671610395, Epsilon: 0.474\n",
      "Episode 149, Total Reward: -167.8331577029777, Epsilon: 0.471\n",
      "Episode 150, Total Reward: -35.48178615659771, Epsilon: 0.469\n",
      "Episode 151, Total Reward: -172.46566964300183, Epsilon: 0.467\n",
      "Episode 152, Total Reward: -35.362773377130324, Epsilon: 0.464\n",
      "Episode 153, Total Reward: -43.197061320612704, Epsilon: 0.462\n",
      "Episode 154, Total Reward: -32.421717427435766, Epsilon: 0.460\n",
      "Episode 155, Total Reward: -32.326260012475004, Epsilon: 0.458\n",
      "Episode 156, Total Reward: 0.40683893369462965, Epsilon: 0.455\n",
      "Episode 157, Total Reward: -192.95849618344744, Epsilon: 0.453\n",
      "Episode 158, Total Reward: -237.80166419981649, Epsilon: 0.451\n",
      "Episode 159, Total Reward: 3.489043276507613, Epsilon: 0.448\n",
      "Episode 160, Total Reward: -62.48907918537505, Epsilon: 0.446\n",
      "Episode 161, Total Reward: -44.191063416030204, Epsilon: 0.444\n",
      "Episode 162, Total Reward: -345.8283576773546, Epsilon: 0.442\n",
      "Episode 163, Total Reward: -5.8814451206750675, Epsilon: 0.440\n",
      "Episode 164, Total Reward: -94.02855079523808, Epsilon: 0.437\n",
      "Episode 165, Total Reward: 29.724978899297582, Epsilon: 0.435\n",
      "Episode 166, Total Reward: -100.80561480212118, Epsilon: 0.433\n",
      "Episode 167, Total Reward: -50.87816533242777, Epsilon: 0.431\n",
      "Episode 168, Total Reward: 14.046069677954506, Epsilon: 0.429\n",
      "Episode 169, Total Reward: -24.239892013218522, Epsilon: 0.427\n",
      "Episode 170, Total Reward: -85.72972761251468, Epsilon: 0.424\n",
      "Episode 171, Total Reward: -26.27759642200411, Epsilon: 0.422\n",
      "Episode 172, Total Reward: 9.739721576819235, Epsilon: 0.420\n",
      "Episode 173, Total Reward: 23.020620540414857, Epsilon: 0.418\n",
      "Episode 174, Total Reward: -105.98893236948915, Epsilon: 0.416\n",
      "Episode 175, Total Reward: 21.79649848532786, Epsilon: 0.414\n",
      "Episode 176, Total Reward: 25.391819003077472, Epsilon: 0.412\n",
      "Episode 177, Total Reward: -26.89143836645424, Epsilon: 0.410\n",
      "Episode 178, Total Reward: 19.11916702324241, Epsilon: 0.408\n",
      "Episode 179, Total Reward: -84.4679198578613, Epsilon: 0.406\n",
      "Episode 180, Total Reward: -94.35123227192278, Epsilon: 0.404\n",
      "Episode 181, Total Reward: -20.92582691091377, Epsilon: 0.402\n",
      "Episode 182, Total Reward: -22.756064117583506, Epsilon: 0.400\n",
      "Episode 183, Total Reward: -116.62895983553412, Epsilon: 0.398\n",
      "Episode 184, Total Reward: 55.30998351292618, Epsilon: 0.396\n",
      "Episode 185, Total Reward: -63.73471686060188, Epsilon: 0.394\n",
      "Episode 186, Total Reward: -42.943346791116085, Epsilon: 0.392\n",
      "Episode 187, Total Reward: -201.64450511210453, Epsilon: 0.390\n",
      "Episode 188, Total Reward: -8.430690021276398, Epsilon: 0.388\n",
      "Episode 189, Total Reward: -36.5587482358648, Epsilon: 0.386\n",
      "Episode 190, Total Reward: -128.62578150789778, Epsilon: 0.384\n",
      "Episode 191, Total Reward: -270.674859437136, Epsilon: 0.382\n",
      "Episode 192, Total Reward: -409.7191298708283, Epsilon: 0.380\n",
      "Episode 193, Total Reward: -26.011644937376534, Epsilon: 0.378\n",
      "Episode 194, Total Reward: -60.0587666791974, Epsilon: 0.376\n",
      "Episode 195, Total Reward: -97.23144287739939, Epsilon: 0.374\n",
      "Episode 196, Total Reward: -46.17909962250801, Epsilon: 0.373\n",
      "Episode 197, Total Reward: -63.33322226804968, Epsilon: 0.371\n",
      "Episode 198, Total Reward: -46.11433820912737, Epsilon: 0.369\n",
      "Episode 199, Total Reward: -42.96804980346528, Epsilon: 0.367\n",
      "Episode 200, Total Reward: -30.9089688076061, Epsilon: 0.365\n",
      "Episode 201, Total Reward: -21.944815357520124, Epsilon: 0.363\n",
      "Episode 202, Total Reward: -30.402578092208746, Epsilon: 0.361\n",
      "Episode 203, Total Reward: -21.821534229932286, Epsilon: 0.360\n",
      "Episode 204, Total Reward: 15.72424558550793, Epsilon: 0.358\n",
      "Episode 205, Total Reward: -33.454606842855064, Epsilon: 0.356\n",
      "Episode 206, Total Reward: 63.78257356262094, Epsilon: 0.354\n",
      "Episode 207, Total Reward: -4.826808289791691, Epsilon: 0.353\n",
      "Episode 208, Total Reward: 47.81632985085963, Epsilon: 0.351\n",
      "Episode 209, Total Reward: -295.8562880370318, Epsilon: 0.349\n",
      "Episode 210, Total Reward: -120.3580619938229, Epsilon: 0.347\n",
      "Episode 211, Total Reward: -40.49325082670698, Epsilon: 0.346\n",
      "Episode 212, Total Reward: 7.8440093094254975, Epsilon: 0.344\n",
      "Episode 213, Total Reward: -64.28178888158078, Epsilon: 0.342\n",
      "Episode 214, Total Reward: -11.42090062335005, Epsilon: 0.340\n",
      "Episode 215, Total Reward: -29.658944272747174, Epsilon: 0.339\n",
      "Episode 216, Total Reward: -75.01444801332248, Epsilon: 0.337\n",
      "Episode 217, Total Reward: 10.586556054035205, Epsilon: 0.335\n",
      "Episode 218, Total Reward: -42.0722358667791, Epsilon: 0.334\n",
      "Episode 219, Total Reward: -22.241703715338062, Epsilon: 0.332\n",
      "Episode 220, Total Reward: 13.356801856245033, Epsilon: 0.330\n",
      "Episode 221, Total Reward: -61.29517016269116, Epsilon: 0.329\n",
      "Episode 222, Total Reward: -3.326595902116004, Epsilon: 0.327\n",
      "Episode 223, Total Reward: -60.104433824008126, Epsilon: 0.325\n",
      "Episode 224, Total Reward: -24.45135586950302, Epsilon: 0.324\n",
      "Episode 225, Total Reward: -28.250917871938498, Epsilon: 0.322\n",
      "Episode 226, Total Reward: -54.320104407072414, Epsilon: 0.321\n",
      "Episode 227, Total Reward: -27.97358103211924, Epsilon: 0.319\n",
      "Episode 228, Total Reward: 10.445727910981587, Epsilon: 0.317\n",
      "Episode 229, Total Reward: 14.84140569918118, Epsilon: 0.316\n",
      "Episode 230, Total Reward: -69.44894254705024, Epsilon: 0.314\n",
      "Episode 231, Total Reward: -78.79027614760267, Epsilon: 0.313\n",
      "Episode 232, Total Reward: -47.98051980531168, Epsilon: 0.311\n",
      "Episode 233, Total Reward: -1.368813011719096, Epsilon: 0.309\n",
      "Episode 234, Total Reward: 24.70296299996606, Epsilon: 0.308\n",
      "Episode 235, Total Reward: -77.28567855375927, Epsilon: 0.306\n",
      "Episode 236, Total Reward: -17.079129333407987, Epsilon: 0.305\n",
      "Episode 237, Total Reward: -12.003612750655037, Epsilon: 0.303\n",
      "Episode 238, Total Reward: -72.310303464147, Epsilon: 0.302\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 45\u001B[0m\n\u001B[1;32m     41\u001B[0m dones \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(dones, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# Calculate the Q-values for the current state and action from the Q-network, and then\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# compute the target Q-values using the target network\u001B[39;00m\n\u001B[0;32m---> 45\u001B[0m q_values \u001B[38;5;241m=\u001B[39m \u001B[43mq_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstates\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgather(\u001B[38;5;241m1\u001B[39m, actions)\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     47\u001B[0m     max_next_q_values \u001B[38;5;241m=\u001B[39m target_network(next_states)\u001B[38;5;241m.\u001B[39mmax(\u001B[38;5;241m1\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/UIUC/Illini-VEX-Robotics/reinforcement-learning/vex-software/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/UIUC/Illini-VEX-Robotics/reinforcement-learning/vex-software/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[5], line 12\u001B[0m, in \u001B[0;36mQNetwork.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 12\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(x))\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc3(x)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### TODO for next meeting\n",
    "- [x] Continue to gain a greater understanding of Deep Q-Learning, along with how and why \n",
    "it makes the landing of the agent on the moon's surface much more smoother\n",
    "    - Specifically focus on \n",
    "        - [x] Backpropogation\n",
    "        - [x] Loss function\n",
    "- [x] Understand the code and why it works\n",
    "- [ ] Perhaps have a brief look at Rainbow DQN??\n",
    "- [ ] Begin implementing our own environment, possibly in Unity with a simple environment of soccer?"
   ],
   "id": "1bf81fc867eb023b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
